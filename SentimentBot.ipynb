{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbM7x-5UEzUR"
   },
   "source": [
    "# Sentiment Analysis using BERT\n",
    "\n",
    "BERT (Bidirectionnal Encoder Representations for Transformers) is a “new method of pre-training language representations” developed by Google and released in late 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6hKNfAlEzUS"
   },
   "source": [
    "### Import Libraries and Set the intial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vezpsX-7GphM",
    "outputId": "a7164402-8a7b-4e4c-e118-ce6498ba4f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prvaddkkepurakkal\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prvaddkkepurakkal\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prvaddkkepurakkal\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\prvaddkkepurakkal\\appdata\\local\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prvaddkkepurakkal\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prvaddkkepurakkal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WtQykqrfEzUT"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "# Torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRINjFWWEzUb"
   },
   "outputs": [],
   "source": [
    "# Set intial variables and constants\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Graph Designs\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# Random seed for reproducibilty\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQOFO5MSEzUf"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6b5ajqzEzUg",
    "outputId": "a7888891-5d69-42b7-a1e0-ccdbe75f04e4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ5Uwg8xEzUk",
    "outputId": "2b23324b-d208-462f-d011-95c2f7847390"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zqGIlGIEzUo"
   },
   "source": [
    "We can see that the most relevant column for us is content and replyContent and the score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24xNLIJlEzUo",
    "outputId": "eae6af5f-068a-480e-acdd-3c78b3e8a81b"
   },
   "outputs": [],
   "source": [
    "# Let's check for missing values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJjld_WSEzUt"
   },
   "source": [
    "There are missing values in some of the columns but Content and score don't have a missing value. We can also look at the class balance. \n",
    "\n",
    "We will be alloting three classes:- \n",
    "1. Positive (Score: 4-5)\n",
    "2. Neutral (Score: 3)\n",
    "3. Negative (Score: 1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWJJv8q7EzUu",
    "outputId": "0a5797e5-60e6-4f35-d37a-50014f8bdb98"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the class balance.\n",
    "sns.countplot(df.score)\n",
    "plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trIBdPvXEzUy"
   },
   "source": [
    "We can see that we have more positive classes than negative and low number of neutral class. I have kept neutral less to focus more on positive and negative classes. Let's allot classes based on scores now. \n",
    "\n",
    "* 0 - negative\n",
    "* 1 - neutral \n",
    "* 2 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyK4bEuhEzUy"
   },
   "outputs": [],
   "source": [
    "# Function to convert score to sentiment\n",
    "def to_sentiment(rating):\n",
    "    \n",
    "    rating = int(rating)\n",
    "    \n",
    "    # Convert to class\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.score.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzaRGLGDEzU2",
    "outputId": "df731374-977a-4f04-ba82-ab60c0c5ffa8"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "ax = sns.countplot(df.sentiment)\n",
    "plt.xlabel('review sentiment')\n",
    "ax.set_xticklabels(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeJSe1hIEzU5"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Machine Learning models don’t work with raw text. You need to convert text to numerical representation. BERT requires even more attention when it comes to this representation. \n",
    "\n",
    "Here are the requirements:\n",
    "\n",
    "* Add special tokens to separate sentences and do classification\n",
    "* Pass sequences of constant length (introduce padding)\n",
    "* Create array of 0s (pad token) and 1s (real token) called attention mask\n",
    "\n",
    "BERT offers a few model architectures and I will be using one of them combined with manual preprocessing. I am using the cased version which considers GREAT and great to be to different entities and BAD might be given more focus than bad.  \n",
    "\n",
    "The tokenizer will break the sentence into words and give numerical values to each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj5eXyqYEzU6",
    "outputId": "4a6b71ec-7503-4884-c669-08383d60225e"
   },
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XqmI4AZEzU9",
    "outputId": "773ac222-7aa8-48d6-eeab-2239646a805a"
   },
   "outputs": [],
   "source": [
    "# Some of the common BERT tokens\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-ridSzLEzVA"
   },
   "source": [
    "BERT works with fixed-length sequences. We’ll use a simple strategy to choose the max length. Let’s store the token length of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBIUYNVoEzVB",
    "outputId": "fbdb58c8-ea3b-44b7-a77f-950c5c18564a"
   },
   "outputs": [],
   "source": [
    "# Store length of each review \n",
    "token_lens = []\n",
    "\n",
    "# Iterate through the content slide\n",
    "for txt in df.content:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpVn-ndyEzVE",
    "outputId": "70db118d-4736-4b61-db2d-bc9de8e0d977"
   },
   "outputs": [],
   "source": [
    "# plot the distribution of review lengths \n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMj924FHEzVI"
   },
   "source": [
    "Most of the reviews seem to contain less than 120 tokens, but we’ll be on the safe side and choose a maximum length of 160. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blPAZJOPEzVJ"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5owK7q5hEzVL"
   },
   "source": [
    "### Preparing Torch Dataset\n",
    "\n",
    "To enter data into a PyTorch, we need a more robust data generator class. We will return the review text as well to validate our predictions easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sbakw3KIEzVM"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    # Constructor Function \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # Length magic method\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    # get item magic method\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # Encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4_AxWjuEzVP"
   },
   "source": [
    "Create a 80% train data and 10% test and 10% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIXH-Y-gEzVQ",
    "outputId": "162ea24f-7bc4-4631-bdc7-8a795334934d"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQeS3OqGEzVS"
   },
   "source": [
    "Create a dataloader to release data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7SCbUlmEzVT"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31-GApKIEzVW"
   },
   "outputs": [],
   "source": [
    "# Create train, test and val data loaders\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZPtu0jjEzVc",
    "outputId": "dd0bb267-6d3d-4ce1-8c81-76b8e4d1beca"
   },
   "outputs": [],
   "source": [
    "# Examples \n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnRQaNQ0EzVf"
   },
   "source": [
    "## Sentiment Classification with BERT and Hugging Face\n",
    "\n",
    "We’ll use the basic BertModel and build our sentiment classifier on top of it. Let’s load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oVuK0UUEzVg",
    "outputId": "354aaaea-b336-41cc-a1de-8a5583545f17"
   },
   "outputs": [],
   "source": [
    "# Load the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v98I1vshEzVi"
   },
   "outputs": [],
   "source": [
    "# Build the Sentiment Classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # Forward propagaion class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        #  Add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXyrki17EzVl"
   },
   "source": [
    "We use a dropout layer for some regularization and a fully-connected layer for our output. We are returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work. Create an instance and move it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eh4ytjBEzVm"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and move to classifier\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqNrk3laEzVr"
   },
   "source": [
    "#### Model Characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFkXhnrLEzVr",
    "outputId": "f065e9d5-9ca5-4a7e-9c20-e953d5da05f1"
   },
   "outputs": [],
   "source": [
    "# Number of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TcSxv4NEzVv"
   },
   "source": [
    "### Training Phase\n",
    "\n",
    "we’ll use the AdamW optimizer provided by Hugging Face. It corrects weight decay. We’ll also use a linear scheduler with no warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEQfhsdnEzVw"
   },
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 10\n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYmCvwhmEzVz"
   },
   "outputs": [],
   "source": [
    "# Function for a single training iteration\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSUZ7gXeEzV2"
   },
   "source": [
    "Write a function to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yil2jCZfEzV2"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZRB5jbrEzV5"
   },
   "source": [
    "Write the training Loop and store the best training state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz0rylI3EzV5",
    "outputId": "545b8419-84b1-4b11-af85-1c45bdc118d9"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h197cMl1Hve2"
   },
   "source": [
    "The above took a lot of time but it's finally working. Now, we can plot the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkOsxRIcHgKt",
    "outputId": "11116244-c94e-43de-f9c5-58dff0c7cc75"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "# Graph chars\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ-hTsMQITml"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q53eNoW2IS7C",
    "outputId": "62b82769-6859-49b8-89c3-3fad47b76763"
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kjQ5tI8IbIl"
   },
   "source": [
    "Define a helper function to get predictions from our models. This is similar to the evaluation function, except that we’re storing the text of the reviews and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2gJhrUHIfsm"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            # Get outouts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18mV-rdaIy-T"
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uvXvTrQVcK6",
    "outputId": "2f139229-c8e1-4b2d-ffd8-63af9520fc0a"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWVfnVW3VgL-",
    "outputId": "ffc9aecb-4868-4e3c-d3b4-e374372ec3f1"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True sentiment')\n",
    "    plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp2juwgkIyiB"
   },
   "source": [
    "This confirms that our model is having difficulty classifying neutral reviews. It mistakes those for negative and positive at a roughly equal frequency.\n",
    "\n",
    "That’s a good overview of the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6AQl1YoEzV8"
   },
   "source": [
    "## Predicting on raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4CWFF0MEzV9"
   },
   "outputs": [],
   "source": [
    "review_text = \"I love completing my todos! Best app ever!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOelFhlsSiK2"
   },
   "outputs": [],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LZm3Ag0WGpi",
    "outputId": "adc38462-7a4a-4554-c856-f8af32c3c8c2"
   },
   "outputs": [],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1056391,
     "sourceId": 1776895,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30042,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
